{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maagnitude/CRC-slides-models/blob/main/CRC-slides-models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2η Εργασία** στο μάθημα **Μηχανική Μάθηση και Εφαρμογές**\n",
        "\n",
        "# **Τμήμα Πληροφορικής και Τηλεματικής - Χαροκόπειο Πανεπιστήμιο**\n",
        "\n",
        "# **Καζάζης Γεώργιος - it214124**\n",
        "\n",
        "Στην παρούσα εργασία θα αναπτύξουμε **μοντέλα Συνελικτικών Νευρωνικών Δικτύων**, για να κατηγοριοποιήσουμε όσο πιο σωστά γίνεται τις 7180 εικόνες μικροσκοπίου, στις 9 κλάσεις που έχουμε.\n",
        "\n",
        "**Αρχίζοντας...**"
      ],
      "metadata": {
        "id": "XtuzDN-a1Fsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Βιβλιοθήκες**\n",
        "Κάνουμε import τα απαραίτητα **modules**. \n",
        "*   Την **pandas** και την **numpy** για την διαχείρηση των δεδομένων μας.\n",
        "\n",
        "*  Την **matplotlib.pyplot** και την **seaborn** για την οπτικοποίηση των δεδομένων μας. **Heatmaps** κλπ.\n",
        "\n",
        "*  Το **tensorflow**, και από αυτό, τα **keras** και **layers** για την ανάπτυξη νευρωνικών δικτύων. \n",
        "  * Επίσης τα **datasets** του **keras** για να χωρίσουμε το **dataset** μας σε **train/validation/test**.\n",
        "\n",
        "*  Το **EarlyStopping** για να εισάγουμε πρόωρο τερματισμό στην εκπαίδευση του μοντέλου, μέσω **callback**.\n",
        "\n",
        "* Επίσης το **Rescaling** για να κάνουμε scale τις τιμές των εικόνων, και το **MaxPooling2D** για να εισάγουμε επίπεδα συγκέντρωσης στο δίκτυο.\n",
        "\n",
        "* Το **EfficientNetB0** για εκπαιδεύσουμε το προεκπαιδευμένο αυτό δίκτυο στo dataset μας.\n",
        "\n",
        "*  Τέλος, κάνουμε import τα **warnings** και τα φιλτράρουμε, ώστε να μην εμφανίζονται.\n"
      ],
      "metadata": {
        "id": "HxNXBPQGRgr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import keras.datasets\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Rescaling\n",
        "from keras.layers import MaxPooling2D\n",
        "\n",
        "from keras.applications.efficientnet import EfficientNetB0\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "metadata": {
        "id": "NKYqBz6t2GIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **drive mount και μεταφόρτωση των εικόνων**"
      ],
      "metadata": {
        "id": "pqYJSf6oSSSg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Et-3ePITcAUu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/drive/MyDrive/Colab Notebooks/CRC_slides.tar.gz' .\n",
        "!tar -xvzf 'CRC_slides.tar.gz'\n",
        "data_dir = '/content/CRC_slides'"
      ],
      "metadata": {
        "id": "EltViBDify6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Υλοποίηση συνάρτησης για την δημιουργία των datasets**\n",
        "Περνάμε τα αρχεία του φακέλου **data_dir** στην μεταβλητή **all_ds** και ύστερα τα χωρίζουμε σε **training**, **validation** και **test sets**. Το πετυχαίνουμε με την χρήση των συναρτήσεων **next** και **skip**, όπου παίρνουμε τον αριθμό των αρχείων που αντιστοιχεί στο **devel_ds** (**next**), ύστερα τα κάνουμε **skip**, ώστε να πάρουμε τα επόμενα για το **train_ds** κτλ.\n",
        "\n",
        "Σχετικά με τις παραμέτρους: \n",
        "* **data_dir** είναι ο φάκελος που βρίσκονται τα αρχεία.\n",
        "* **labels='inferred'** για να παράξει τα labels από τους υποφακέλους.\n",
        "* **label_moded='int'** που συμαίνει ότι τα labels είναι κωδικοποιημένα ως integers, και θα χρησιμοποιήσουμε \"**sparse_categorical_crossentropy**\".\n",
        "  * Στην αρχή το είχα βάλει '**categorical**' και τα μοντέλα μου τρέχανε, με loss='**categorical_crossentropy**' αλλά το **EfficientNetB0** δεν έτρεχε, οπότε το γύρισα σε '**int**', και πλέον τρέχει και αυτό, και τα δικά μου με **sparse**.\n",
        "* **class_names=None** για να χρησιμοποιηθεί αλφαριθμητικη σειρά, μιας και δεν μας νοιάζει.\n",
        "* **color_mode='rgb'** για να έχουν 3 κανάλια οι εικόνες. (default value)\n",
        "* **batch_size** βάζουμε το batch size που μας δίνει η εκφώνηση. Πόσες εικόνες θα πάρει μαζί. Το default θα ήταν 32.\n",
        "* **image_size** βάζουμε το size που μας δίνει η εκφώνηση. Αν βάλουμε άλλο αριθμό απ αυτόν που είναι η εικόνες, γίνεται resize.\n",
        "* **shuffle=True** που είναι και η default τιμή, απλά το βάζουμε να φαίνεται. Αν ήταν False θα έκανε sort τα δεδομένα σε αλφαριθμητική σειρά.\n",
        "* **seed=123** ως σπόρο για το shuffling."
      ],
      "metadata": {
        "id": "hY2VWJuH1GLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(data_dir, train_pct=0.6, val_pct=0.2, test_pct=0.2, batch_size=64, img_size=(224, 224)):\n",
        "\n",
        "  # Δημιουργία του training set\n",
        "  train_ds = keras.utils.image_dataset_from_directory(data_dir, labels='inferred', label_mode='int', class_names=None, \n",
        "                              color_mode='rgb', validation_split=val_pct+test_pct, subset='training',  batch_size=batch_size, \n",
        "                              image_size=img_size, shuffle=True, seed=123)\n",
        "  # Δημιουργία του αρχικού validation-test set\n",
        "  val_ds = keras.utils.image_dataset_from_directory(data_dir, labels='inferred', label_mode='int', class_names=None, \n",
        "                              color_mode='rgb', validation_split=val_pct+test_pct, subset='validation',  batch_size=batch_size, \n",
        "                              image_size=img_size, shuffle=True, seed=123)\n",
        "\n",
        "  # Χωρίζουμε το val_ds στα batches του, και ύστερα βάζουμε τα πρώτα μισά στο testing set, και τα υπόλοιπα στο validation set\n",
        "  val_batches = tf.data.experimental.cardinality(val_ds)\n",
        "  test_ds = val_ds.take(val_batches // 2)\n",
        "  val_ds = val_ds.skip(val_batches // 2)\n",
        "\n",
        "  # Το development set είναι ολόκληρο το dataset, χωρίς το test set\n",
        "  devel_ds = tf.data.Dataset.concatenate(train_ds, val_ds)\n",
        "\n",
        "  # Παίρνουμε τις κατηγορίες των δεδομένων\n",
        "  classes = train_ds.class_names\n",
        "\n",
        "  return devel_ds, train_ds, val_ds, test_ds, classes"
      ],
      "metadata": {
        "id": "FzSuEqhYgN6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "devel_ds, train_ds, val_ds, test_ds, classes = load_dataset(data_dir)"
      ],
      "metadata": {
        "id": "-cFTHhHHTGVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Για να δούμε τις πρώτες 9 εικόνες του train_ds**"
      ],
      "metadata": {
        "id": "_XxDS2OLN-1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in devel_ds.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(classes[labels[i]])\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "yN8BWvQVNcL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Εδώ τυπώνουμε το πόσες εικόνες έχει το κάθε batch, το μέγεθος κάθε εικόνας με τον αριθμό καναλιών, καθώς επίσης και τον αριθμό των labels που έχει το κάθε batch."
      ],
      "metadata": {
        "id": "JmjyE1liWrJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for image, label in train_ds.take(1):\n",
        "  print(\"Image shape: \", image.numpy().shape)\n",
        "  print(\"Label: \", label.numpy().shape)"
      ],
      "metadata": {
        "id": "pKwcPMfAm9Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes= len(classes)"
      ],
      "metadata": {
        "id": "_NjznWTvX8wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Παρακάτω οπτικοποιούμε τον αριθμό των αρχείων κάθε κατηγορίας στο σύνολο ανάπτυξης**.\n",
        "\n",
        "Παρατηρούμε ότι η κατανομή των κατηγοριών στο σύνολο δεδομένων μας είναι αρκετά **ανισόρροπη**, με ορισμένες κατηγορίες (**1η, 4η, 5η και 9η**) να έχουν σημαντικά περισσότερα δείγματα από άλλες (**3η, 6η, 7η και 8η**). Αυτό μπορεί να είναι προβληματικό κατά την εκπαίδευση μοντέλων μηχανικής μάθησης, καθώς το μοντέλο μπορεί να καταλήξει να υπερεκπαιδευτεί (**overfitting**) στις πιο διαδεδομένες κατηγορίες και να μην γενικεύει καλά στις κατηγορίες που υποεκπροσωπούνται."
      ],
      "metadata": {
        "id": "P9YX8Y_oVoxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.concatenate([y for x, y in devel_ds])\n",
        "plt.hist(y, list(range(num_classes + 1)))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WX4DXOrNVRiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.concatenate([y for x, y in train_ds])\n",
        "plt.hist(y, list(range(num_classes + 1)))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nBno96VcaE91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.concatenate([y for x, y in test_ds])\n",
        "plt.hist(y, list(range(num_classes + 1)))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NIROk8cTaE4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.concatenate([y for x, y in val_ds])\n",
        "plt.hist(y, list(range(num_classes + 1)))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EaYIwsZmaIIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Υλοποίηση συνάρτησης για την δημιουργία συνελικτικού νευρωνικού δικτύου**.\n",
        "\n",
        "Αρχικά περνάμε στην είσοδο την εικόνα **224x224 pixel x3** για το rgb.\n",
        "\n",
        "Στο **πρώτο layer** κάνουμε rescaling τις τιμές των εικόνων (pixel values) από **[0, 255]** σε **[0, 1]**.\n",
        "\n",
        "Στο επόμενο έχουμε ένα συνελικτικό επίπεδο **8 φίλτρων** 3x3 με **padding='same'** για να έχουμε έξοδο ίση με την είσοδο, και activation function **ReLU**.\n",
        "\n",
        "Ύστερα έχουμε ένα επίπεδο **MaxPooling** με βήμα 2, βάζοντας **pool_size=(2, 2)** όπου αυτή θα είναι και η default τιμή των **strides** από τη στιγμή που δεν τους βάζουμε άλλη τιμή εμείς.\n",
        "\n",
        "Επαναλαμβάνουμε τα δύο παραπάνω επίπεδα, με την διαφορά ότι το συνελικτικό τώρα έχει **16 φίλτρα**.\n",
        "\n",
        "Ένα επίπεδο **Flatten()** για να μετατρέψουμε τις τιμές κάθε εικόνας σε διάνυσμα.\n",
        "\n",
        "Τέλος, έχουμε ένα πλήρως συνδεδεμένο επίπεδο **32 νευρώνων** με activation function **ReLU**, κι ένα επίπεδο εξόδου για τις **9** (num_classes) **κατηγορίες** που θέλουμε να προβλέψουμε, με activation function **softmax** για να πάρουμε τιμές που αθροίζουν στο 1 (για το πόσο σίγουρο είναι το μοντέλο για κάθε κατηγορία)."
      ],
      "metadata": {
        "id": "B8ubiTK6cMOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cnn1(num_classes):\n",
        "\n",
        "  model = keras.Sequential([\n",
        "      keras.Input(shape=(224, 224, 3)),\n",
        "      layers.Rescaling(1./255),\n",
        "      layers.Conv2D(8, kernel_size=(3, 3), padding='same', activation='relu'),\n",
        "      layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "      layers.Conv2D(16, kernel_size=(3, 3), padding='same', activation='relu'),\n",
        "      layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(32, activation='relu'),\n",
        "      layers.Dense(num_classes, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "gzSWEo0_2b9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Εκπαίδευση μοντέλου και εκτίμηση.**\n",
        "Καλούμε την παραπάνω συνάρτηση και μας επιστρέφει το μοντέλο μας.\n",
        "\n",
        "Το κάνουμε compile με Adam optimizer, βάζοντας τις επιθυμητές τιμές στις παραμέτρους. **Ρυθμό εκμάθησης=0.001**, **ρυθμό ενημέρωσης πρώτης ροπής=0.9** και **ρυθμό ενημέρωσης δεύτερης ροπής=0.99**. Για loss επέλεξα την **Sparse categorical crossentropy**, για να μην κάνω one-hot encoding στα labels.\n",
        "\n",
        "Ρυθμίζουμε την εκπαίδευση του μοντέλου να σταματήσει πρόωρα αν δεν παρουσιαστεί μείωση της απώλειας στο σύνολο επικύρωσης για 5 εποχές, με την κλήση της **EarlyStopping** δίνοντας της ως παράμετρο την τιμή που πρέπει να παρακολουθεί και τις εποχές πέραν των οποιών θα πρέπει να σταματήσει η εκπαίδευση. Την περνάμε ως **callback** στην fit.\n",
        "\n",
        "Τέλος, εκπαιδεύουμε το μοντέλο και το εξετάζουμε στο σύνολο επικύρωσης."
      ],
      "metadata": {
        "id": "EUh99HoCZRIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = cnn1(num_classes)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.99),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "history = model.fit(train_ds, validation_data=val_ds, batch_size=64, epochs=20, callbacks=[early_stopping])\n",
        "score = model.evaluate(val_ds)"
      ],
      "metadata": {
        "id": "MEDNZvXh2bvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Αποτελέσματα**\n",
        "Την πρώτη φορά το μοντέλο εκπαιδεύτηκε για **42 λεπτά**, για τις **18 εποχές** μιας και χρειάστηκε να τερματιστεί πρόωρα, κι αυτό γιατί απ την 13η εποχή που είχε **val_loss: 0.4994**, στις επόμενες 5 εποχές δεν κατάφερε να πέσει κάτω απ αυτή την τιμή (η οποία ήταν η χαμηλότερη, από κάθε άλλη). Ωστόστο η εξέταση στο σύνολο επικύρωσης έφερε τα τελικά αποτελέσματα **loss: 0.5732** και **accuracy: 0.7985**\n",
        "\n",
        "Επειδή χρειάστηκε να ξανακάνω κάποια πράγματα, έπρεπε να το ξανατρέξω, αυτή τη φορά με την **TPU** του **Google Colab**. Έτρεξε και τις **20 εποχές**, σε **47 λεπτά**. Τα αποτελέσματα ήταν **loss: 0.4546** και **accuracy: 0.8586**, αρκετά παραπάνω από την προηγούμενη φορά. Ας δούμε τώρα πως θα τα πάει στο **test set**."
      ],
      "metadata": {
        "id": "uhuDWak1cPoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def confusion_matrix(model, test_ds):\n",
        "    y_test = []\n",
        "    y_pred = []\n",
        "    for x_1, y_1 in test_ds:\n",
        "        y_pred_1 = model.predict(x_1)\n",
        "        y_test.append(y_1)\n",
        "        y_pred.append(y_pred_1)\n",
        "        y_true = np.concatenate(y_test)\n",
        "        y_p = np.concatenate(y_pred)\n",
        "        y_hat = tf.argmax(y_p, axis=1)\n",
        "        cm = tf.math.confusion_matrix(y_true, y_hat)\n",
        "    return cm, y_hat, y_true"
      ],
      "metadata": {
        "id": "hV1PrllgtfU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Υλοποίηση συνάρτησης για τον σχεδιασμό του Confusion Matrix**\n",
        "\n",
        "Με χρήση ενός heatmap θα παρουσιάσουμε τα αποτελέσματα του υπολογισμένου confusion matrix, όπου βάζουμε τις παραμέτρους annot=True και annot_kws={'size':20} για να εμφανίσουμε τις τιμές σε κάθε περιοχή του πίνακα, και να έχουν μέγεθος 20. Επίσης, η παράμετρος **fmt=\"d\"** είναι για να εμφανίζονται οι τιμές ως ακέραιοι. (Γιατί χωρίς αυτήν, το 466 εμφανιζόταν ως 4.7e+02)\n",
        "\n",
        "Τέλος, με την **axis.tick_top()** βάζουμε τα labels του άξονα x στην κορυφή του plot."
      ],
      "metadata": {
        "id": "5Rmu8BLlgYEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def heat_confmatrix (cm):\n",
        "  \n",
        "  labels = classes\n",
        "\n",
        "  # Δίνουμε τα labels στον πίνακα\n",
        "  cm_plt = pd.DataFrame(cm, index = labels, columns = labels)\n",
        "  \n",
        "  plt.subplots(figsize=(9, 7))\n",
        "  \n",
        "  ax = sns.heatmap(cm_plt, cmap='viridis', annot=True, annot_kws={'size':20}, fmt=\"d\")\n",
        "\n",
        "  ax.xaxis.tick_top()\n",
        "  ax.set_title(\"Confusion Matrix\\n\")\n",
        "  plt.ylabel('True')\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "ItIn65SnyohF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Παίρνουμε και το **y_true** ώστε να υπολογίσουμε παρακάτω την μετρική **f1_score**."
      ],
      "metadata": {
        "id": "drq8AmMa8aA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm, y_hat, y_true = confusion_matrix(model, test_ds)"
      ],
      "metadata": {
        "id": "VyPEctuItqZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **f1_score**\n",
        "Θα χρησιμοποιήσουμε την μετρική **F1 score**, δίνοντας της ως παραμέτρους τις προβλέψεις **y_hat**, τις πραγματικές τιμές **y_true** και το average='micro' ώστε να υπολογιστεί συνολικά μετρώντας το σύνολο των **αληθώς θετικών**, των **ψευδώς αρνητικών** και των **ψευδώς θετικών** αποτελεσμάτων."
      ],
      "metadata": {
        "id": "x0EYjRjTkPtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1_score(y_true, y_hat, average='micro')\n",
        "\n",
        "# (Διαγώνιος / Όλες τις τιμές) -> 1181 / 1408 = ~0.83877"
      ],
      "metadata": {
        "id": "ekPgJUD2kWtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ορθότητα μοντέλου**\n",
        "Η ορθότητα του μοντέλου μας στο σύνολο **εκπαίδευσης** έφτασε μέχρι κα το **0.9942**. Στην τελευταία εποχή ήταν **0.9870**. Στο σύνολο **επικύρωσης** έφτασε το **0.8586** και στο σύνολο **δοκιμής** το **~0.8388**, όπως υπολογίσαμε με την **f1_score**."
      ],
      "metadata": {
        "id": "lxe1CP4figIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Confusion Matrix**\n",
        "Στις παρακάτω **4 κατηγορίες**, ενώ λόγω της **ανισορροπίας** περιμέναμε κακές προβλέψεις, το μοντέλο τα πάει αναπάντεχα καλά.\n",
        "\n",
        "* Παρατηρούμε ότι για την κατηγορία **STR** δεν την έχει μάθει καλά, μιας και μάντεψε σωστά σε **34 δείγματα**, και μάντεψε λανθασμένα σε (7+1+4+14+8+16): **50 δείγματα**, τις κατηγορίες DEB, LYM, MUC, MUS, NORM, TUM αντίστοιχα, δίνοντας ποσοστό ακρίβειας (34 / 84): **0.4047**\n",
        "\n",
        "* Την **DEB** την πέτυχε σε **54 δείγματα** και την μπέρδεψε με άλλες σε 14 δείγματα, δίνοντας ποσοστό (54 / 68): ~**0.7941**\n",
        "\n",
        "* Στην **MUS** πέφτει σε **83 δείγματα** μέσα, και μπερδεύει μόνο 4 για DEB, 20 για STR, και 3 για TUM δίνοντας ποσοστό ακρίβειας (83 / 110): ~**0.7545**\n",
        "\n",
        "* Τέλος, στην κατηγορία **NORM** τα πάει επίσης καλύτερα από άλλη φορά που το έτρεξα, καθώς προβλέπει **98 δείγματα** σωστά, και στα 26 δείγματα προβλέπει TUM, και στα υπόλοιπα (1+1+7+1+7): **17 δείγματα**, προβλέπει αντίστοιχα DEB, LYM, MUC, MUS και STR. Ποσοστό ακρίβειας (98 / 141): ~**0.695**\n",
        "\n",
        "Την προηγούμενη φορά που το έτρεξα (και είχε τερματίσει στην **18η εποχή** λόγω **Early_stopping**) είχε ποσοστά ακρίβειας -> **STR: ~0.217**, **DEB: ~0.507**, **MUS: ~0.854** και **NORM: ~0.276**\n",
        "\n",
        "Οι κατηγορίες που παρατηρήθηκε το μικρότερο ποσοστό ακρίβειας (κυρίως στην προηγούμενη εκπαίδευση), είναι οι ίδιες με αυτές που παρατηρήσαμε ότι υπάρχουν σε μικρή ποσότητα σε όλα τα sets (**3η, 6η, 7η και 8η**), που σημαίνει ότι πολύ πιθανόν το μοντέλο **υποεκπαιδεύτηκε** (**underfitting**) σε αυτές.\n",
        "\n",
        "Τέλος, στις κατηγορίες **ADI**, **BACK**, **MUC** και **TUM** (**1η, 4η, 5η και 9η**), τα πήγε πολύ καλά, μιας και όπως είδαμε και πιο πάνω, υπάρχουν πολλά δείγματα αυτών των κατηγοριών στο dataset, με αποτέλεσμα το μοντέλο να τις μάθει πολύ καλύτερα απ' τις άλλες, και ακόμα να μπερδεύει πολλές φορές και τις άλλες με αυτές, όπως παρατηρήσαμε."
      ],
      "metadata": {
        "id": "EGbCfceOhFJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "heat_confmatrix(cm.numpy())"
      ],
      "metadata": {
        "id": "7_MNyoeOxcuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Εναλλακτική μετρική**\n",
        "Αν δούμε τα δείγματα ως: \n",
        "\n",
        "[**ADI**, **BACK**, **DEB**, **LYM**, **MUC**, **MUS**, **NORM** -> **NORMAL**] και [**STR**, **TUM** -> **NOT NORMAL**]\n",
        "\n",
        "Ο πίνακας σύγχυσης θα είναι πλέον **2x2** και σύμφωνα με υπολογισμούς που έκανα, θα έχει τις τιμές:\n",
        "\n",
        "                       PREDICTIONS\n",
        "                   NORMAL  / NOT NORMAL\n",
        "                /----------|------------\\\n",
        "         NORMAL |   1007   |     69     |            \n",
        "                |----------|------------| TRUE\n",
        "     NOT NORMAL |    72    |    260     |\n",
        "                \\----------|------------/\n",
        "\n",
        "Εμάς ουσιαστικά μας ενδιαφέρουν οι τιμές **69** και **72**. Σημαίνουν πως σε 69 δείγματα είχαμε υγιή ιστό και το μοντέλο πρόβλεψε ότι δεν είναι υγιής, ενώ στα 72 δεν είχαμε υγιή ιστό και το μοντέλο πρόβλεψε ότι είναι υγιής.\n",
        "\n",
        "Άρα σε ένα ποσοστό (72 / 141): ~**0.5106** κάνει λάθος το οποίο οδηγεί στην 1η περίπτωση όπου ένας **μη υγιής** ιστός, θα **θεωρηθεί υγιής** και δεν θα προχωρήσει σε θεραπεία.\n",
        "\n",
        "και\n",
        "\n",
        "σε ένα ποσοστό ~**0.4893** κάνει λάθος το οποίο οδηγεί στην 2η περίπτωση όπου ένας ιστός **υγιής** θα **θεωρηθεί μη υγιής** και θα προχωρήσει σε θεραπεία.\n",
        "\n",
        "Με λίγα λόγια, αν κάνει λάθος σχετικά με το αν ένας ιστός είναι υγιής ή όχι, στην 1η περίπτωση θεωρεί **λανθασμένα** κάποιον **υγιή** με ποσοστό **0.5106** το οποίο είναι **μεγαλύτερο** απ' το ποσοστό **0.4893**, της 2ης περίπτωσης, όπου θεωρεί **λανθασμένα** κάποιον **μη υγιή**, και αυτό είναι αρκετά **αρνητικό** με την έννοια ότι το να χρειαστεί εσφαλμένα να **θεραπεύσουμε έναν ήδη υγιή** είναι **λιγότερο σημαντικό** από το, εσφαλμένα, να **μην θεραπεύσουμε έναν μη υγιή**. Αυτό που θα θέλαμε λοιπόν, είναι, το ποσοστό της 1ης περίπτωσης να είναι όσο μικρότερο γίνεται από αυτό της 2ης. Αν σας έπιασε πονοκέφαλος, απολογούμαι.\n",
        "\n",
        "Δεν γνωρίζω αν είναι σωστό το σκεπτικό μου με τον πίνακα, και αν είναι **σωστή** και **τεκμηριωμένη** η εξήγηση, αλλά έτσι το σκέφτηκα, με βοήθεια και από το παράδειγμα που δόθηκε στο μάθημα της **Μηχανικής Μάθησης** (στις 13/12/2022)\n",
        "\n",
        "**Σημ**: Λογικά θα υπάρχει συνάρτηση που θα το υπολογίζει κατευθείαν. Δεν την βρήκα όμως και επειδή είχα αυτόν τον πίνακα στο μυαλό μου, είπα να το φτιάξω έτσι.\n"
      ],
      "metadata": {
        "id": "1HFoBLchObZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Deeper CNN**\n",
        "Το παρακάτω μοντέλο υλοποιήθηκε όπως το προηγούμενο, με την διαφορά ότι έχει **περισσότερα επίπεδα**, των οποίων τα φίλτρα είναι **3x3**, έχουν **padding='same'** για να χουμε ίδιο μέγεθος εξόδου με την είσοδο, και activation function την **ReLU**, και είναι τα εξής:\n",
        "* 2 επίπεδα **32** φίλτρων\n",
        "* 1 επίπεδο συγκέντρωσης Max Pooling με βήμα 4\n",
        "* 2 επίπεδα **64** φίλτρων\n",
        "* 1 επίπεδο συγκέντρωσης Max Pooling με βήμα 2\n",
        "* 2 επίπεδα **128** φίλτρων\n",
        "* 1 επίπεδο συγκέντρωσης Max Pooling με βήμα 2\n",
        "* 3 επίπεδα **256** φίλτρων\n",
        "* 1 επίπεδο συγκέντρωσης Max Pooling με βήμα 2\n",
        "* 1 επίπεδο **512** φίλτρων\n",
        "* 1 επίπεδο συγκέντρωσης Max Pooling με βήμα 2\n",
        "* 1 επίπεδο μετατροπής σε 1 διάσταση\n",
        "* 1 πλήρως συνδεδεμένο επίπεδο **1024** νευρώνων\n",
        "* Τέλος, ένα επίπεδο με εξόδους τις κατηγορίες μας και activation function **softmax**."
      ],
      "metadata": {
        "id": "3LWddVfC2cKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cnn2(num_classes):\n",
        "\n",
        "  model = keras.Sequential([\n",
        "      keras.Input(shape=(224, 224, 3)),\n",
        "      layers.Rescaling(1./255),\n",
        "      layers.Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu'),\n",
        "      layers.Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu'),      \n",
        "      layers.MaxPooling2D(pool_size=(4, 4)),\n",
        "      layers.Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'),\n",
        "      layers.Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'),      \n",
        "      layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "      layers.Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu'),\n",
        "      layers.Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu'),  \n",
        "      layers.MaxPooling2D(pool_size=(2, 2)),   \n",
        "      layers.Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'),\n",
        "      layers.Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'),  \n",
        "      layers.Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'),  \n",
        "      layers.MaxPooling2D(pool_size=(2, 2)),  \n",
        "      layers.Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu'),               \n",
        "      layers.MaxPooling2D(pool_size=(2, 2)),  \n",
        "      layers.Flatten(),\n",
        "      layers.Dense(1024, activation='relu'),\n",
        "      layers.Dense(num_classes, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "j4MlZOHc2a6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Εκτελέστηκαν οι 17 εποχές σε **3 ώρες και 16 λεπτά**, και τερματίστηκε. Προφανώς δεν τερμάτισε λόγω του **val_loss** μιας και δεν παρατηρείτε να πέρασαν 5 εποχές από την χαμηλότερη τιμή (**0.3434**). Αφήνω το κελί αυτό με τα αποτελέσματα του, και θα το ξανατρέξω πιο κάτω μπας και ολοκληρωθούν οι 20 εποχές για να είμαστε πιο ακριβείς. \n"
      ],
      "metadata": {
        "id": "acwMjvamjcmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = cnn2(num_classes)\n",
        "\n",
        "model2.summary()\n",
        "\n",
        "model2.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.99),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "history = model2.fit(train_ds, validation_data=val_ds, batch_size=64, epochs=20, callbacks=[early_stopping])\n",
        "score = model2.evaluate(val_ds)"
      ],
      "metadata": {
        "id": "gTvKUftH3u0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = cnn2(num_classes)\n",
        "\n",
        "model3.summary()\n",
        "\n",
        "model3.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.99),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "history = model3.fit(train_ds, validation_data=val_ds, batch_size=64, epochs=20, callbacks=[early_stopping])\n",
        "score = model3.evaluate(val_ds)"
      ],
      "metadata": {
        "id": "N0SgmHKydxxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Τι παρατηρούμε για την επίδοση**\n",
        "Για αρχή να σημειωθεί ότι τη δεύτερη φορά που το έτρεξα, τερματίστηκε πρόωρα στην **15η εποχή**, μετά από **4μιση ώρες** με **TPU**, λόγω του ότι μετά την **10η εποχή** όπου είχε **val_loss: 0.5189**, δεν ξαναέπεσε κάτω από αυτή την τιμή για τις επόμενες 5 εποχές.\n",
        "\n",
        "Η παρατήρηση που μπορούμε να κάνουμε εδώ είναι ότι μεταξύ αυτού του δικτύου και του προηγούμενου, **υπάρχει διαφορά στην επίδοση**. Ακόμα και να συνέχιζε αυτό το μοντέλο, μπορούμε να δούμε ότι γενικά το **validation accuracy** του **ανεβοκατέβαινε** λίγο. Ενώ του προηγούμενου μοντέλου, ανέβαινε σχετικά σταθερά. Ακόμα και την πρώτη φορά που είχα τρέξει το προηγούμενο, και είχε accuracy **0.7985** ανέβαινε πάλι **σταθέρα**, και πάλι τα πήγε καλύτερα από αυτό εδώ το μοντέλο (και είχε σταματήσει επίσης πρόωρο στην **18η εποχή**).\n",
        "\n",
        "Αυτό θεωρώ ότι αφείλεται στο ότι το μοντέλο από ένα σημείο και μετά **υπερεκπαιδεύεται** (**overfitting**) στις κατηγορίες που έχει σε μεγάλη ποσότητα, μην μπορώντας να γενικεύσει, με αποτέλεσμα όσο βαθύ και να το κάνουμε να μην αυξάνεται η επίδοση του σχετικά με το πρώτο που είχε πολύ λιγότερα επίπεδα και φίλτρα. Επίσης του πρώτου η εκπαίδευση διήρκησε και **~4 ώρες λιγότερο**."
      ],
      "metadata": {
        "id": "1zSLB2jolevW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm, y_hat, y_true = confusion_matrix(model3, test_ds)\n"
      ],
      "metadata": {
        "id": "14Puo60wOgOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Confusion Matrix του δεύτερου μοντέλου**\n",
        "\n",
        "Παρακάτω έχουμε το **confusion matrix** και το **f1_score** του μοντέλου μας στο **test set**, το οποίο είναι όσο κακό το περιμέναμε. "
      ],
      "metadata": {
        "id": "fmnunYQ9PjBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1_score(y_true, y_hat, average='micro')"
      ],
      "metadata": {
        "id": "pg8EaoQBPPw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "heat_confmatrix(cm.numpy())"
      ],
      "metadata": {
        "id": "RL0cuV8GPgXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Εκπαίδευση προεκπαιδευμένου μοντέλου, στο dataset μας.**\n",
        "Έτρεξε για **1 ώρα και 55 λεπτά**, τις 5 εποχές, και μας έδωσε accuracy πολύ καλύτερο απ τα προηγούμενα μοντέλα που έτρεξαν για 20 εποχές\n",
        "\n",
        "**loss: 0.2828 accuracy: 0.9440**\n",
        "\n",
        "Οπότε παρατηρούμε πως όντως ένα **πιο βαθύ δίκτυο** το οποίο είχε **5,330,571** παραμέτρους σε αντίθεση με τους **7,671,337** που είχε το προηγούμενο μοντέλο μας, εκπαιδεύεται πολύ καλύτερα λόγω βάθους (**μεγάλου αριθμού επιπέδων**) αλλά **κυρίως** λόγω της **εμπειρίας** του πάνω σε **image datasets**. Αυτό γιατί το **EfficientNetB0** έχει προ-εκπαιδευτεί σε ένα μεγάλο σύνολο δεδομένων και έχει ρυθμιστεί λεπτομερώς στη συγκεκριμένη εργασία που προσπαθούμε να επιλύσουμε. Αυτό σημαίνει ότι έχει ήδη ένα ισχυρό θεμέλιο γνώσης και μπορεί να είναι σε θέση να μάθει γρηγορότερα και να αποδώσει καλύτερα από ένα μοντέλο που έχει αρχικοποιηθεί τυχαία."
      ],
      "metadata": {
        "id": "fxkj0p7l5feC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model4 = keras.applications.efficientnet.EfficientNetB0()\n",
        "\n",
        "model4.summary()"
      ],
      "metadata": {
        "id": "R87KJScDBW9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.99),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history = model4.fit(train_ds, validation_data=val_ds, batch_size=32, epochs=5)\n",
        "score = model4.evaluate(val_ds)"
      ],
      "metadata": {
        "id": "WUC2IQ_NaFgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm, y_hat = confusion_matrix(model4, test_ds)"
      ],
      "metadata": {
        "id": "ItbmPGRALMf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Confusion Matrix των προβλέψεων του EfficientNetB0**\n",
        "Πέρα από 29 δείγματα που τα πρόβλεψε ως **NORM**, ενώ ήταν **ADI**, και τα 22 δείγματα που τα πρόβλεψε ως **MUS** ενώ ήταν **STR**, σε όλα τα άλλα, εκτός ελάχιστων αστοχιών, τα έχει πάει καταπληκτικά.\n",
        "\n",
        "Το accuracy του στο test set είναι (1318 / 1408): **0.936**\n",
        "\n",
        "\n",
        "\n",
        "**Σημ**: Για να μην το ξανατρέχει επειδή το ολοκλήρωσα άλλη μέρα, έκανα την πράξη της διαίρεσης της διαγωνίου με το σύνολο, με το χέρι."
      ],
      "metadata": {
        "id": "U1huPMo4cmXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "heat_confmatrix(cm.numpy())"
      ],
      "metadata": {
        "id": "Bj33vKSMLTQB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}